{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGiefYf210M0"
      },
      "source": [
        "!mkdir test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYgDiK0Anfix"
      },
      "source": [
        "!pip3 install symspellpy\r\n",
        "!pip3 install tqdm\r\n",
        "!pip3 install nltk\r\n",
        "!sudo apt-get install rdfind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7K1j4L3ym2S"
      },
      "source": [
        "!wget -q \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron1.tar.gz\"\r\n",
        "!wget -q \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron2.tar.gz\"\r\n",
        "!wget -q \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron3.tar.gz\"\r\n",
        "!wget -q \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron4.tar.gz\"\r\n",
        "!wget -q \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron5.tar.gz\"\r\n",
        "!wget -q \"http://nlp.cs.aueb.gr/software_and_datasets/Enron-Spam/preprocessed/enron6.tar.gz\"\r\n",
        "!wget -q \"http://nlp.cs.aueb.gr/software_and_datasets/lingspam_public.tar.gz\"\r\n",
        "!wget -q \"https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\"\r\n",
        "!wget -q \"https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\"\r\n",
        "!wget -q \"https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\"\r\n",
        "!wget -q \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\"\r\n",
        "!wget -q \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\"\r\n",
        "!wget -q \"https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\"\r\n",
        "!wget -q \"https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\"\r\n",
        "!wget -q \"https://spamassassin.apache.org/old/publiccorpus/20030228_spam_2.tar.bz2\"\r\n",
        "!tar -xzf enron1.tar.gz\r\n",
        "!tar -xzf enron2.tar.gz\r\n",
        "!tar -xzf enron3.tar.gz\r\n",
        "!tar -xzf enron4.tar.gz\r\n",
        "!tar -xzf enron5.tar.gz\r\n",
        "!tar -xzf enron6.tar.gz\r\n",
        "!tar -xzf lingspam_public.tar.gz\r\n",
        "\r\n",
        "!tar -xjf 20021010_easy_ham.tar.bz2\r\n",
        "!mv easy_ham 20021010_easy_ham\r\n",
        "\r\n",
        "!tar -xjf 20021010_hard_ham.tar.bz2\r\n",
        "!mv hard_ham 20021010_hard_ham\r\n",
        "\r\n",
        "!tar -xjf 20021010_spam.tar.bz2\r\n",
        "!mv spam 20021010_spam\r\n",
        "\r\n",
        "!tar -xjf 20030228_easy_ham.tar.bz2\r\n",
        "!mv easy_ham 20030228_easy_ham\r\n",
        "\r\n",
        "!tar -xjf 20030228_easy_ham_2.tar.bz2\r\n",
        "!mv easy_ham_2 20030228_easy_ham_2\r\n",
        "\r\n",
        "!tar -xjf 20030228_hard_ham.tar.bz2\r\n",
        "!mv hard_ham 20030228_hard_ham\r\n",
        "\r\n",
        "!tar -xjf 20030228_spam.tar.bz2\r\n",
        "!mv spam 20030228_spam\r\n",
        "\r\n",
        "!tar -xjf 20030228_spam_2.tar.bz2\r\n",
        "!mv spam_2 20030228_spam_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6XcayZryrZ3"
      },
      "source": [
        "rm enron1.tar.gz enron2.tar.gz enron3.tar.gz enron4.tar.gz enron5.tar.gz enron6.tar.gz lingspam_public.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krwYkuFBKJFG"
      },
      "source": [
        "rm 20021010_easy_ham.tar.bz2 20021010_hard_ham.tar.bz2 20021010_spam.tar.bz2 20030228_easy_ham.tar.bz2 20030228_easy_ham_2.tar.bz2 20030228_hard_ham.tar.bz2 20030228_spam.tar.bz2 20030228_spam_2.tar.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utckgbbz9d1o"
      },
      "source": [
        "!rdfind -deleteduplicates true ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlvsNRqplqT3"
      },
      "source": [
        "!find . -name \"cmds\" -delete\r\n",
        "!find . -name \"0000.7b1b73cf36cf9dbc3d64e3f2ee2b91f1\" -delete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7_5biJRpUu4"
      },
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sort BoW in descending order\n",
        "import heapq\n",
        "\n",
        "# Process raw html files\n",
        "import re\n",
        "from html import unescape\n",
        "import email\n",
        "import email.policy\n",
        "from email.parser import BytesParser, Parser\n",
        "from email.policy import default\n",
        "\n",
        "# Give time count for processing certain tasks\n",
        "import tqdm\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "\n",
        "# Give root folder where CS20S020_assignment3_dataset.zip is extracted.\n",
        "# Set this to '/content/' if running on colab\n",
        "PATH = '/content/'\n",
        "global PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0tnt4Hg7Dzt"
      },
      "source": [
        "np.random.seed(11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMEa31dPUzR1"
      },
      "source": [
        "# Import raw dataset\n",
        "def import_datasets():\n",
        "\n",
        "  # All unprocessed enron and lingspam emails are directly converted into a pandas dataframe.\n",
        "    \n",
        "  rows = []\n",
        "  spam_count = 0\n",
        "  ham_count = 0\n",
        "\n",
        "  enron_files = ['enron1', 'enron2', 'enron3', 'enron4', 'enron5', 'enron6']\n",
        "  path = PATH\n",
        "\n",
        "  for enron_file in enron_files:\n",
        "    for directories, subdirs, files in os.walk(path + enron_file):\n",
        "      for filename in files:\n",
        "        with open(os.path.join(directories, filename), encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "          data = f.read()\n",
        "          if (os.path.split(directories)[1]  == 'ham'):\n",
        "            rows.append({'email': data, 'class': 'ham'})\n",
        "            ham_count += 1\n",
        "    \n",
        "          if (os.path.split(directories)[1]  == 'spam'):\n",
        "            rows.append({'email': data, 'class': 'spam'})\n",
        "            spam_count += 1\n",
        "\n",
        "  print(\"Distribution of Enron emails\")\n",
        "  print(\"#Ham emails: \", ham_count, \"#Spam emails: \", spam_count)\n",
        "  \n",
        "  spam_count = 0\n",
        "  ham_count = 0\n",
        "  \n",
        "  path = PATH + '/lingspam_public/bare/'\n",
        "  lingspam_folders = ['part1', 'part2', 'part3', 'part4', 'part5', 'part6',\n",
        "                    'part7', 'part8', 'part9', 'part10']\n",
        "\n",
        "  for folder in lingspam_folders:\n",
        "    for directories, subdirs, text_files in os.walk(path + folder):\n",
        "      for filename in text_files:\n",
        "        with open(os.path.join(directories, filename), encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "          data = f.read()\n",
        "          if 'spmsg' in filename:\n",
        "            rows.append({'email': data, 'class': 'spam'})\n",
        "            spam_count += 1\n",
        "          else:\n",
        "            rows.append({'email': data, 'class': 'ham'})\n",
        "            ham_count += 1\n",
        "\n",
        "  print(\"Distribution of Lingspam emails\")\n",
        "  print(\"#Ham emails: \", ham_count, \"#Spam emails: \", spam_count)\n",
        "\n",
        "  spamassasino = ['20021010_easy_ham/', '20021010_hard_ham/', '20021010_spam/', '20030228_easy_ham/', '20030228_easy_ham_2/', \n",
        "              '20030228_hard_ham/', '20030228_spam/', '20030228_spam_2/', '20050311_spam_2/']\n",
        "  path = PATH\n",
        "  index = []\n",
        "\n",
        "  spam_count = 0\n",
        "  ham_count = 0\n",
        "\n",
        "  for folder in spamassasino:\n",
        "    folder_path = path + folder\n",
        "    for directories, subdirs, files in os.walk(folder_path):\n",
        "      for filename in files:\n",
        "        with open(os.path.join(folder_path, filename), \"rb\") as f:\n",
        "          try:\n",
        "            #data = BytesParser(policy=default).parse(f)\n",
        "            data = email.message_from_binary_file(f)\n",
        "            \n",
        "            text = \"\"\n",
        "            emtext = \"\"\n",
        "            \n",
        "            if data.is_multipart() or 'multipart' in data.get_content_type():\n",
        "              html = \"\"\n",
        "\n",
        "              for part in data.get_payload():\n",
        "                  if part.get_content_type() == 'text/plain' or part.get_content_type() == 'text/html':\n",
        "                    \n",
        "                    if part.get_content_charset() is None:\n",
        "                      charset = chardet.detect(str(part))['encoding']\n",
        "                    else:\n",
        "                      charset = part.get_content_charset()\n",
        "\n",
        "                    if part.get_content_type() == 'text/plain':\n",
        "                      text += str(part.get_payload(decode=True), str(charset), 'ignore')\n",
        "                    if part.get_content_type() == 'text/html':\n",
        "                      text = htmlTOtext(part.get_payload())\n",
        "                      html += text\n",
        "              \n",
        "              data = \"\"\n",
        "              if html != \"\" and text != \"\":\n",
        "                emtext += html.strip() + '\\n' + text.strip()\n",
        "                data = emtext #.encode('utf-8', 'ignore')\n",
        "\n",
        "              elif html == \"\" and text == \"\":\n",
        "                data = None\n",
        "\n",
        "              else:\n",
        "                if html == \"\":\n",
        "                  emtext = text.strip()\n",
        "                else:\n",
        "                  emtext = html.strip()\n",
        "                data = emtext #.encode('utf-8', 'ignore')\n",
        "\n",
        "            else:\n",
        "              if data.get_content_type() == 'text/html':\n",
        "                text = htmlTOtext(data.get_payload())\n",
        "                #text = text.encode('utf-8', errors='ignore')\n",
        "              elif data.get_content_type() == 'text/plain':\n",
        "                if data.get_charsets() == [None]:\n",
        "                  text = str(data.get_payload(decode=True), 'utf-8', 'ignore') #.encode('utf-8','ignore')\n",
        "                else:\n",
        "                  text = str(data.get_payload(decode=True), data.get_content_charset(), 'ignore') #.encode('utf-8','ignore')\n",
        "              data = text.strip()\n",
        "\n",
        "            if data is not None:\n",
        "              if 'easy_ham' in folder or 'easy_ham_2' in folder or 'hard_ham' in folder:\n",
        "                rows.append({'email': data, 'class': 'ham'})\n",
        "                ham_count += 1\n",
        "\n",
        "              if 'spam' in folder or 'spam_2' in folder:\n",
        "                rows.append({'email': data, 'class': 'spam'})\n",
        "                spam_count += 1\n",
        "              index.append(folder_path+filename)\n",
        "          except:\n",
        "            continue\n",
        "\n",
        "  print(\"Distribution of Spamassasin emails\")\n",
        "  print(\"#Ham emails: \", ham_count, \"#Spam emails: \", spam_count)\n",
        "  return rows, index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w01-2Dutdsym"
      },
      "source": [
        "def clean_email(emails_array):\n",
        "    \"\"\" Remove all punctuation, urls, numbers, and newlines.\n",
        "    Convert to lower case.\"\"\"\n",
        "\n",
        "    i = 0\n",
        "    for email in emails_array:\n",
        "      if type(email) == 'bytes':\n",
        "        email = str(email, encoding='ascii', errors='ignore')\n",
        "      else:\n",
        "        email = str(email)\n",
        "      email = email.lower()\n",
        "      email = re.sub(r'\\\\r', ' ', email)\n",
        "      email = re.sub(r'\\\\n', ' ', email)\n",
        "      email = re.sub(r'http\\S+', 'httplink', email)\n",
        "      email = re.sub(r'[^\\s]+@[^\\s]+', 'emailaddr', email)\n",
        "      email = re.sub(r'[^\\s]+\\.(gif|png|jpg|jpeg)$', 'imgext', email)\n",
        "      email = re.sub(r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b', 'ipaddr', email)\n",
        "      email = re.sub(r'[^\\S]', ' ', email)\n",
        "      email = re.sub(r' +', ' ', email)\n",
        "      #The '$' sign gets replaced with 'dollar'\n",
        "      email = re.sub('[$]+', 'dollar', email)\n",
        "\n",
        "      email = email.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "      \n",
        "      email = re.sub(\"\\d+\", ' ', email)\n",
        "      email = email.replace('\\n', ' ')\n",
        "\n",
        "      email = email.strip()\n",
        "      emails_array[i] = email\n",
        "      i += 1\n",
        "\n",
        "    return emails_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EHLKpzP_7f8"
      },
      "source": [
        "def count_all_words(emails_array):\r\n",
        "  # Word frequency\r\n",
        "  word2count = {}\r\n",
        "  for email in emails_array:\r\n",
        "    words = email.split()\r\n",
        "    for word in words:\r\n",
        "      if word not in word2count.keys():\r\n",
        "        word2count[word] = 1\r\n",
        "      else:\r\n",
        "        word2count[word] += 1\r\n",
        "\r\n",
        "  return word2count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqQCc32ZAQsL"
      },
      "source": [
        "def remove_stopwords(emails_array):\r\n",
        "  # open file and read the content in a list. \r\n",
        "  # Remove stopwords such as 'the','you', 'an' etc.\r\n",
        "  # Also, removes words of length < 2 or length > 15.\r\n",
        "  \r\n",
        "  with open(PATH + 'stopwords.txt', 'r') as f:\r\n",
        "    sw = []\r\n",
        "    for line in f:\r\n",
        "      sw.append(line[:-1])\r\n",
        "\r\n",
        "  i = 0\r\n",
        "  for email in emails_array:\r\n",
        "    words = email.split()\r\n",
        "    email = \"\"\r\n",
        "    for word in words:\r\n",
        "      if word not in sw and len(word) > 2 and len(word) < 15:\r\n",
        "        email += word + \" \"\r\n",
        "    if email != \"\":\r\n",
        "      email = re.sub(r'[^\\S]', ' ', email)\r\n",
        "      email = re.sub(r' +', ' ', email)\r\n",
        "      email = email.strip()\r\n",
        "      emails_array[i] = email\r\n",
        "    i += 1\r\n",
        "\r\n",
        "  return emails_array  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x5g1KUQ6Bi7"
      },
      "source": [
        "def lemmatize_stem(emails_array, train=True):\n",
        "  # Words are stemmized and lemmatized\n",
        "  i = 0\n",
        "  for email in emails_array:\n",
        "\n",
        "    # Create the stemmer, lemmatizer\n",
        "    wnl = WordNetLemmatizer()\n",
        "    s_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "    # Split text into words.\n",
        "    words = email.split()\n",
        "    email = \"\"\n",
        "    for word in words:\n",
        "        x = s_stemmer.stem(word)\n",
        "        x = wnl.lemmatize(x)\n",
        "        email += x + \" \"\n",
        "    emails_array[i] = email\n",
        "    i += 1\n",
        "\n",
        "  emails_array = remove_stopwords(emails_array)\n",
        "\n",
        "  # Remove low frequency words only for train emails\n",
        "  if train:\n",
        "    emails_array = remove_low_freq_words(emails_array)    \n",
        "\n",
        "  return emails_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4q4pQBsK34Y"
      },
      "source": [
        "def remove_low_freq_words(emails_array):\r\n",
        "\r\n",
        "  # Get frequency for all words in our dataset\r\n",
        "  word2count = count_all_words(emails_array)\r\n",
        "\r\n",
        "  # Remove all words with freq <= 3\r\n",
        "  i = 0\r\n",
        "  for email in emails_array:\r\n",
        "    words = email.split()\r\n",
        "    email = \"\"\r\n",
        "    for word in words:\r\n",
        "      if word2count[word] >= 3:\r\n",
        "        email += word + \" \"\r\n",
        "    email = re.sub(r'[^\\S]', ' ', email)\r\n",
        "    email = re.sub(r' +', ' ', email)\r\n",
        "    email = email.strip()\r\n",
        "    \r\n",
        "    emails_array[i] = email\r\n",
        "    i += 1\r\n",
        "    \r\n",
        "  return emails_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPCPU2xgfmqO"
      },
      "source": [
        "def spell_check(emails_array, labels):\r\n",
        "  \r\n",
        "  emails_array = remove_stopwords(emails_array)\r\n",
        "\r\n",
        "  # Removing low frequency words\r\n",
        "  emails_array = remove_low_freq_words(emails_array)\r\n",
        "  \r\n",
        "  nspam_emails = emails_array[np.where(labels == 0)]\r\n",
        "  word2count = count_all_words(nspam_emails) # count_all_words(emails_array)\r\n",
        "  \r\n",
        "  dist = 3\r\n",
        "  dictionary_path = PATH + \"/frequency_dictionary_en_82_765.txt\"\r\n",
        "  sym_spell = SymSpell(max_dictionary_edit_distance=dist, prefix_length=dist+1, count_threshold=1)\r\n",
        "  sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\r\n",
        "\r\n",
        "  # finding correctly spelled words\r\n",
        "  spell_correct_all_words = {}\r\n",
        "  for word in tqdm.tqdm(word2count.keys()):\r\n",
        "    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=dist)\r\n",
        "\r\n",
        "    if len(suggestions) > 0:\r\n",
        "      suggestion = str(suggestions[0]).split(',')[0]\r\n",
        "      if word != suggestion:\r\n",
        "        spell_correct_all_words[word] = suggestion\r\n",
        "    else:\r\n",
        "      spell_correct_all_words[word] = '0'\r\n",
        "\r\n",
        "  # Removing no spell found words and correcting words\r\n",
        "  i = 0\r\n",
        "  for email in emails_array:\r\n",
        "    words = email.split()\r\n",
        "    for word in words:\r\n",
        "      if (word in spell_correct_all_words.keys()) and spell_correct_all_words[word] != '0':\r\n",
        "        email = email.replace(word, spell_correct_all_words[word])\r\n",
        "      if (word in spell_correct_all_words.keys()) and spell_correct_all_words[word] == '0':\r\n",
        "        email = email.replace(word, \"\")\r\n",
        "    \r\n",
        "    email = re.sub(r'[^\\S]', ' ', email)\r\n",
        "    email = re.sub(r' +', ' ', email)\r\n",
        "    email = email.strip()\r\n",
        "    \r\n",
        "    emails_array[i] = email\r\n",
        "    i += 1\r\n",
        "  \r\n",
        "  emails_array = remove_stopwords(emails_array)\r\n",
        "\r\n",
        "  return emails_array, spell_correct_all_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3Wsma6tZ38S"
      },
      "source": [
        "# Creating the Bag of Words model\r\n",
        "def create_bag_of_words(emails_array, labels, max_words):\r\n",
        "  spam_words = {}\r\n",
        "  nspam_words = {}\r\n",
        "\r\n",
        "  spam_emails = emails_array[np.where(labels == 1)]\r\n",
        "  nspam_emails = emails_array[np.where(labels == 0)]\r\n",
        "\r\n",
        "  spam_words = count_all_words(spam_emails)\r\n",
        "  nspam_words = count_all_words(nspam_emails)\r\n",
        "\r\n",
        "  # If there are common keys in both spam and non-spam list, then we assign the key\r\n",
        "  # to that list which has a higher frequency\r\n",
        "  common_words = set(spam_words.keys()).intersection(set(nspam_words.keys()))\r\n",
        "  if len(common_words) > 0:\r\n",
        "    for key in common_words:\r\n",
        "      if spam_words[key] > nspam_words[key]:\r\n",
        "        nspam_words.pop(key)\r\n",
        "      else:\r\n",
        "        spam_words.pop(key)\r\n",
        "      \r\n",
        "  top_k_spam = heapq.nlargest(int(max_words/2), spam_words, key=spam_words.get)\r\n",
        "  top_k_nspam = heapq.nlargest(int(max_words/2), nspam_words, key=nspam_words.get)\r\n",
        "  freq_words = top_k_spam + top_k_nspam\r\n",
        "  \r\n",
        "  return freq_words, spam_words, nspam_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmHg50bdiXz1"
      },
      "source": [
        "def tokenize(emails_array, freq_words, tf=False):\r\n",
        "  # Tokenize words based on BoW. This method gives the term-frequency in\r\n",
        "  # binarized form if tf is False, or raw term-frequency if tf is True\r\n",
        "\r\n",
        "  X = []\r\n",
        "  for email in emails_array:\r\n",
        "    vector = []\r\n",
        "    email_tokenization = email.split()\r\n",
        "    for word in freq_words:\r\n",
        "      if word in email_tokenization:\r\n",
        "        if tf:\r\n",
        "          c = 0\r\n",
        "          for w in freq_words:\r\n",
        "            if word in w:\r\n",
        "              c += 1\r\n",
        "          vector.append(c)\r\n",
        "        else:\r\n",
        "          vector.append(1)\r\n",
        "      else:\r\n",
        "        vector.append(0)\r\n",
        "    X.append(vector)\r\n",
        "  X = np.asarray(X)\r\n",
        "\r\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCArlmaQnFuS"
      },
      "source": [
        "def htmlTOtext(html):\r\n",
        "    # Converts raw html email into plain text\r\n",
        "\r\n",
        "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\r\n",
        "    text = re.sub('<a\\s.*?>', ' ', text, flags=re.M | re.S | re.I)\r\n",
        "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\r\n",
        "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\r\n",
        "    return unescape(text) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp3Uy8czcuLH"
      },
      "source": [
        "def remove_empty_emails(emails_array):\r\n",
        "  # Empty emails are removed from training set.  \r\n",
        "\r\n",
        "  if emails_array.ndim > 1:\r\n",
        "    ems = emails_array[:, 0]\r\n",
        "  elif emails_array.ndim == 1:\r\n",
        "    ems = emails_array\r\n",
        "  \r\n",
        "  i = 0\r\n",
        "  idxs = []\r\n",
        "  for em in ems:\r\n",
        "    if len(em) == 0:\r\n",
        "      idxs.append(i)\r\n",
        "    i += 1\r\n",
        "\r\n",
        "  if len(idxs) > 0:\r\n",
        "    emails_array = np.delete(emails_array, np.array(idxs), axis=0)\r\n",
        "    \r\n",
        "  return emails_array, idxs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsL1fuEWf3Dl"
      },
      "source": [
        "def remove_empty_tokenized_emails(token_emails, emails_array):\r\n",
        "  # Removes zero-vector tokenized emails during training\r\n",
        "\r\n",
        "  idxs = np.where(np.sum(token_emails, axis=1) == 0)[0]\r\n",
        "  \r\n",
        "  if len(idxs) > 0:\r\n",
        "    token_emails = np.delete(token_emails, idxs, axis=0)\r\n",
        "    emails_array = np.delete(emails_array, idxs, axis=0)\r\n",
        "\r\n",
        "  return token_emails, emails_array, idxs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI5ONnulLelM"
      },
      "source": [
        "# Extract test emails for evaluation.\r\n",
        "# 'Upload emails in test folder. Drag and drop'\r\n",
        "def get_test_emails(path):\r\n",
        "  folder_path = path\r\n",
        "\r\n",
        "  rows = []\r\n",
        "\r\n",
        "  for f in os.listdir(folder_path):\r\n",
        "    with open(folder_path + f, encoding=\"utf-8\", errors=\"ignore\") as f:\r\n",
        "      data = f.read()\r\n",
        "      rows.append({'email': data})\r\n",
        "\r\n",
        "  return rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1ZWHnwo5McK"
      },
      "source": [
        "class BernNB(object):\r\n",
        "    def __init__(self, alpha=1.0, binarize=0.5):\r\n",
        "        self.alpha = alpha\r\n",
        "        self.binarize = binarize\r\n",
        "\r\n",
        "    def fit(self, X, y):\r\n",
        "        X = self.binarize_X(X)\r\n",
        "\r\n",
        "        num_samples = X.shape[0]\r\n",
        "\r\n",
        "        # Group by labels\r\n",
        "        group_by_label = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\r\n",
        "\r\n",
        "        # Label prior\r\n",
        "        self.class_log_prior_ = [np.log(len(i) / num_samples) for i in group_by_label]\r\n",
        "\r\n",
        "        # Each word's count\r\n",
        "        word_count = np.array([np.array(i).sum(axis=0) for i in group_by_label]) + self.alpha\r\n",
        "\r\n",
        "        smoothing = 2 * self.alpha\r\n",
        "\r\n",
        "        # number of documents in each class + smoothing\r\n",
        "        num_docs = np.array([len(i) + smoothing for i in group_by_label])\r\n",
        "        \r\n",
        "        # probability of each word\r\n",
        "        self.feature_prob_ = word_count / num_docs[np.newaxis].T\r\n",
        "        return self\r\n",
        "\r\n",
        "    def predict_log_probs(self, X):\r\n",
        "        X = self.binarize_X(X)\r\n",
        "        return [(np.log(self.feature_prob_) * x + \\\r\n",
        "                 np.log(1 - self.feature_prob_) * np.abs(x - 1)\r\n",
        "                ).sum(axis=1) + self.class_log_prior_ for x in X]\r\n",
        "\r\n",
        "    def predict(self, X):\r\n",
        "        X = self.binarize_X(X)\r\n",
        "        return np.argmax(self.predict_log_probs(X), axis=1)\r\n",
        "    \r\n",
        "    def binarize_X(self, X):\r\n",
        "      return np.where(X > self.binarize, 1, 0) if self.binarize != None else X\r\n",
        "\r\n",
        "    def score(self, pred, true):\r\n",
        "      return sum(pred == true) / len(true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5euBQUdvcLDa"
      },
      "source": [
        "# Download and concatenate dataset into a dataframe\r\n",
        "print(\"Importing datasets....\")\r\n",
        "rows, index = import_datasets()\r\n",
        "\r\n",
        "emails_dataset = pd.DataFrame(rows)\r\n",
        "\r\n",
        "# Map 'spam' to 1 and 'ham' to 0.\r\n",
        "emails_dataset['class'] = emails_dataset['class'].map({'spam':1, 'ham':0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6__LcLvnjnNS"
      },
      "source": [
        "emails = emails_dataset.to_numpy()\r\n",
        "train_emails = emails\r\n",
        "\r\n",
        "# Clean emails\r\n",
        "print(\"Cleaning training emails....\")\r\n",
        "train_emails[:, 0] = clean_email(train_emails[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk0nipwS4XHd"
      },
      "source": [
        "print(\"Performing spellcheck....\")\r\n",
        "# Perform spell check on non-spam words\r\n",
        "train_emails[:, 0], spell_correct_all_words = spell_check(train_emails[:, 0], train_emails[:, 1])\r\n",
        "\r\n",
        "# Remove stopwords\r\n",
        "train_emails[:, 0] = remove_stopwords(train_emails[:, 0])\r\n",
        "\r\n",
        "print(\"Stemming and Lemmatizing....\")\r\n",
        "# Stemmatize and Lemmatize all words in all emails\r\n",
        "train_emails[:, 0] = lemmatize_stem(train_emails[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvbfMEKReP44"
      },
      "source": [
        "# Remove empty emails\r\n",
        "train_emails, _ = remove_empty_emails(train_emails)\r\n",
        "\r\n",
        "# Shuffle training set randomly\r\n",
        "np.random.shuffle(train_emails)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Nt9zP9JMVA"
      },
      "source": [
        "# Get BoW\n",
        "print(\"Creating BoW....\")\n",
        "BoW, spam_words, nspam_words = create_bag_of_words(train_emails[:, 0], train_emails[:, 1], 5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_LHkgGRVmHC"
      },
      "source": [
        "# Vectorize words\r\n",
        "tokenized_emails = tokenize(train_emails[:, 0], BoW)\r\n",
        "\r\n",
        "# Remove zero-vectors emails from training set\r\n",
        "tokenized_emails, train_emails, _ = remove_empty_tokenized_emails(tokenized_emails, train_emails)\r\n",
        "\r\n",
        "# Laplace smoothing\r\n",
        "tokenized_emails = np.append(tokenized_emails, [np.ones(len(BoW))], 0)\r\n",
        "tokenized_emails = np.append(tokenized_emails, [np.ones(len(BoW))], 0)\r\n",
        "X_train = tokenized_emails\r\n",
        "\r\n",
        "# Define the target labels\r\n",
        "y_train = train_emails[:, 1]\r\n",
        "y_train = np.append(y_train, 0)\r\n",
        "y_train = np.append(y_train, 1)\r\n",
        "y_train = y_train.astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0UjRZzWDV7M"
      },
      "source": [
        "path = PATH + '/test/'\r\n",
        "# Read test emails\r\n",
        "rows = get_test_emails(path)\r\n",
        "test_df = pd.DataFrame(rows)\r\n",
        "\r\n",
        "test_ems = test_df['email'].values\r\n",
        "# Clean test emails and remove stopwords\r\n",
        "test_ems = clean_email(test_ems)\r\n",
        "test_ems = remove_stopwords(test_ems)\r\n",
        "_, idxs1 = remove_empty_emails(test_ems)\r\n",
        "\r\n",
        "# Vectorize words\r\n",
        "tokenized_tests = tokenize(test_ems, BoW)\r\n",
        "\r\n",
        "# Remove zero-vectors emails from test set\r\n",
        "_, _, idxs2 = remove_empty_tokenized_emails(tokenized_tests, test_ems)\r\n",
        "\r\n",
        "X_test = tokenized_tests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zyt61_02nAP"
      },
      "source": [
        "bnb = BernNB(alpha=1, binarize=0.5).fit(X_train, y_train)\r\n",
        "test_predictions = bnb.predict(X_test)\r\n",
        "\r\n",
        "# Setting zero-vector emails by default to ham or label '0'\r\n",
        "if len(idxs1) > 0:\r\n",
        "  test_predictions[idxs1] = 0\r\n",
        "if len(idxs2) > 0:\r\n",
        "  test_predictions[idxs2] = 0\r\n",
        "\r\n",
        "# Output returned to test folder\r\n",
        "with open(path+'output.txt', 'w', newline='') as file_output:\r\n",
        "  for item in test_predictions:\r\n",
        "    file_output.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}